# Prepare Data for ML APIs on Google Cloud: Challenge Lab

## Challenge scenario

As a junior data engineer in Jooli Inc. and recently trained with Google Cloud and a number of data services you have been asked to demonstrate your newly learned skills. The team has asked you to complete the following tasks.

You are expected to have the skills and knowledge for these tasks so don't expect step-by-step guides.

## Task 1. Run a simple Dataflow job

In this task, you use the Dataflow batch template Text Files on Cloud Storage to BigQuery under "Process Data in Bulk (batch)" to transfer data from a Cloud Storage bucket (gs://cloud-training/gsp323/lab.csv). The following table has the values you need to correctly configure the Dataflow job.

You will need to make sure you have:

* Create a BigQuery dataset called **`lab_255`** with a table called **`customers_145`**.
* Create a Cloud Storage Bucket called **`qwiklabs-gcp-01-fc93c6843fbe-marking`**.

|Field|Value|
|-----|-----|
|Cloud Storage input file(s)|gs://cloud-training/gsp323/lab.csv|
|Cloud Storage location of your BigQuery schema file|gs://cloud-training/gsp323/lab.schema|
|BigQuery output table|**`qwiklabs-gcp-01-fc93c6843fbe:lab_255.customers_145`**|
|Temporary directory for BigQuery loading process|**`gs://qwiklabs-gcp-01-fc93c6843fbe-marking/bigquery_temp`**|
|Temporary location|**`gs://qwiklabs-gcp-01-fc93c6843fbe-marking/temp`**|
|Optional Parameters > JavaScript UDF path in Cloud Storage|gs://cloud-training/gsp323/lab.js|
|Optional Parameters > JavaScript UDF name|	transform|
|Optional Parameters > Machine Type|e2-standard-2|

### Solution Task 1:

* Navigation menu > Cloud Storage > Buckets

* Create a **Storage** Bucket > Enter name as your GCP Project: **`qwiklabs-gcp-01-fc93c6843fbe-marking`** > Leave others to default > Create

* Go to **BigQuery** > Select project ID > Create Dataset > Enter the name as **`lab_255`** and click on Create

* Run the following from the Cloud Shell:

```yaml
gsutil cp gs://cloud-training/gsp323/lab.csv .

cat lab.csv

gsutil cp gs://cloud-training/gsp323/lab.schema .

cat lab.schema
```

* Now, create a table `customers_145` inside the `lab_255` dataset and configure it as follows:

![Create Table](/Google%20Cloud%20Computing%20Foundations/images/challenge-4-create-table.png)

* Click on **Create table**

* Go to **Dataflow** > Jobs > Create Job from Template

![Create Job](/Google%20Cloud%20Computing%20Foundations/images/challenge-4-create-job.png)

* Run the Job (may take a few minutes).

## Task 2. Run a simple Dataproc job

In this task, you run an example Spark job using Dataproc.

Before you run the job, log into one of the cluster nodes and copy the /data.txt file into hdfs (use the command `hdfs dfs -cp gs://cloud-training/gsp323/data.txt /data.txt`).

Run a Dataproc job using the values below.

|Field|Value|
|-----|-----|
|Region|**`us-central1`**|
|Job type|Spark|
|Main class or jar|org.apache.spark.examples.SparkPageRank|
|Jar files|file:///usr/lib/spark/examples/jars/spark-examples.jar|
|Arguments|/data.txt|
|Max restarts per hour|1|
|Dataproc Cluster|Compute Engine|
|Region|**`us-central1`**|
|Machine Series|E2|
|Manager Node|Set Machine Type to e2-standard-2|
|Worker Node|Set Machine Type to e2-standard-2|
|Max Worker Nodes|2|
|Primary disk size|100 GB|
|Internal IP only|Deselect "Configure all instances to have only internal IP addresses|

### Solution Task 2:

* Go to **Dataproc** > Clusters > Create Cluster

![Create Data Proc 1](/Google%20Cloud%20Computing%20Foundations/images/challenge-4-create-data-proc1.png)

![Create Data Proc 2](/Google%20Cloud%20Computing%20Foundations/images/challenge-4-create-data-proc2.png)

* DonÂ´t forget in the section `Customize cluster`, Deselect `"Configure all instances to have only internal IP addresses`

* Create (leave the other sections by default)

* Select the Created Cluster > Go to **VM Instances** > **SSH** into cluster

* Run the following command: 

```yaml
hdfs dfs -cp gs://cloud-training/gsp323/data.txt /data.txt
```

* Exit the **SSH**

* Submit Job > Configure as given:

![Submit a Job](/Google%20Cloud%20Computing%20Foundations/images/challenge-4-submit-job.png)

* Click on **SUBMIT**, (may take a few minutes).

## Task 3. Use the Google Cloud Speech-to-Text API

Use Google Cloud Speech-to-Text API to analyze the audio file `gs://cloud-training/gsp323/task3.flac`. Once you have analyzed the file, upload the resulting file to: **`gs://qwiklabs-gcp-01-fc93c6843fbe-marking/task3-gcs-207.result`**

**Note:** If you are facing issues in this task, you can refer to the respective lab for troubleshooting: `Google Cloud Speech-to-Text API: Qwik Start`

### Solution Task 3:

* Create an API Key, click **Navigation menu** > **APIs & services** > **Credentials**.

* Then click Create credentials.

* In the drop down menu, select **API key**.

* Copy the key you just generated and click Close

* Go to **Compute Engine** and select any of the instance created in the previous taks, click on **SSH**

* In the command line, enter in the following, replacing **<YOUR_API_KEY>** with the API key you copied from previously generated:

```yaml
export API_KEY=<YOUR_API_KEY>
```

* Create `request.json` in the SSH command line. You'll use this to build your request to the Speech-to-Text API:

```yaml
touch request.json
```

* Open the request.json:

```yaml
nano request.json
```

* Add the following to your request.json file, using the uri value of the sample raw audio file:

```yaml
{
  "config": {
      "encoding":"FLAC",
      "languageCode": "en-US"
  },
  "audio": {
      "uri":"gs://cloud-training/gsp323/task3.flac"
  }
}
```

* Press control + x and then y to save and click Enter to close the `request.json` file.

* Pass your request body, along with the API key environment variable, to the Speech-to-Text API with the following curl command (all in one single command line):

```yaml
curl -s -X POST -H "Content-Type: application/json" --data-binary @request.json \
"https://speech.googleapis.com/v1/speech:recognize?key=${API_KEY}"
```

* Run the following command to save the response in a `result.json` file.

```yaml
curl -s -X POST -H "Content-Type: application/json" --data-binary @request.json \
"https://speech.googleapis.com/v1/speech:recognize?key=${API_KEY}" > result.json
```

* Copy the **result.json** into the buscket.

```yaml
gsutil cp result.json gs://qwiklabs-gcp-01-fc93c6843fbe-marking/task3-gcs-207.result
```

## Task 4. Use the Cloud Natural Language API
Use the Cloud Natural Language API to analyze the sentence from text about Odin. The text you need to analyze is `"Old Norse texts portray Odin as one-eyed and long-bearded, frequently wielding a spear named Gungnir and wearing a cloak and a broad hat."` Once you have analyzed the text, upload the resulting file to: **`gs://qwiklabs-gcp-01-fc93c6843fbe-marking/task4-cnl-910.result`**

**Note:** If you are facing issues in this task, you can refer to the respective lab for troubleshooting: `Cloud Natural Language API: Qwik Start`

### Solution Task 4:

* In the same VM, we don't need other API key.

* Run the following gcloud command:

```yaml
gcloud ml language analyze-entities --content="Old Norse texts portray Odin as one-eyed and long-bearded, frequently wielding a spear named Gungnir and wearing a cloak and a broad hat." > result.json
```

* Run the below command to preview the output of result.json file:

```yaml
cat result.json
```

* Copy the **result.json** into the buscket.

```yaml
gsutil cp result.json gs://qwiklabs-gcp-01-fc93c6843fbe-marking/task4-cnl-910.result
```



